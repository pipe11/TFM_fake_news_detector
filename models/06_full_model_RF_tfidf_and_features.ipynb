{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fouth model: Random forest with TFIDF + complexity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize  \n",
    "from nltk.data import load  \n",
    "from nltk.stem import SnowballStemmer  \n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/pipe11/TFM_fake_news_detector/data/corpus_features.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spanish tokenization\n",
    "\n",
    "    - Spanish stopwords\n",
    "    - Stems\n",
    "    - Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword list to use\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "#Spanish stemmer:\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):  \n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "#Punctuation to remove\n",
    "non_words = list(punctuation)\n",
    "\n",
    "#Adding spanish punctuation\n",
    "non_words.extend(['¿', '¡'])  \n",
    "non_words.extend(map(str,range(10)))\n",
    "\n",
    "def tokenize(text):  \n",
    "    #Remove punctuation\n",
    "    text = ''.join([c for c in text if c not in non_words])\n",
    "    #Tokenize\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    #Stem\n",
    "    try:\n",
    "        stems = stem_tokens(tokens, stemmer)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(text)\n",
    "        stems = ['']\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(  \n",
    "                analyzer = 'word',\n",
    "                tokenizer = tokenize,\n",
    "                lowercase = True,\n",
    "                stop_words = spanish_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pipe11/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "df_vectorized = tfidf_vectorizer.fit_transform(df['Text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dense dataframe Sparse, and Combine with TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['Labels'] = labelencoder.fit_transform(df['Category'])\n",
    "y = df['Labels']\n",
    "\n",
    "# combine TF-IDF features with Dense features as a dataframe Sparse\n",
    "categorical_features = ['sentences', 'n_words', 'avg_words_sent', 'avg_word_size', 'TTR']\n",
    "\n",
    "X = hstack([csr_matrix(df[categorical_features].values), df_vectorized[0:]])\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into testing set, training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': True, \n",
    "    'max_depth': 95, \n",
    "    'max_features': 'auto', \n",
    "    'min_samples_leaf': 1, \n",
    "    'min_samples_split': 4, \n",
    "    'n_estimators': int(1800)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8366197183098592"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier(bootstrap = True, max_depth = 95, max_features = 'auto', min_samples_leaf = 1, \n",
    "                            min_samples_split = 4, n_estimators = 1800)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_rf.predict(X_test)\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pipe11/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=95, min_samples_split=4, n_estimators=1800)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/pipe11/TFM_fake_news_detector/data/corpus_spanish.csv', index_col = 0)\n",
    "\n",
    "# Label encoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['Labels'] = labelencoder.fit_transform(df['Category'])\n",
    "y = df['Labels']\n",
    "\n",
    "df_text = pd.DataFrame()\n",
    "df_text['Text'] = text\n",
    "\n",
    "df = pd.concat([df['Text'], df_text], axis = 0)\n",
    "\n",
    "########## TFIDF-Vectorizer ##########\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(  \n",
    "            analyzer = 'word',\n",
    "            tokenizer = tokenize,\n",
    "            lowercase = True,\n",
    "            stop_words = spanish_stopwords)\n",
    "\n",
    "df_vectorized = tfidf_vectorizer.fit_transform(df['Text']) \n",
    "\n",
    "########## Combine features ##########\n",
    "\n",
    "# label encoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['Labels'] = labelencoder.fit_transform(df['Category'])\n",
    "y = df['Labels']\n",
    "\n",
    "# combine TF-IDF features with Dense features as a dataframe Sparse\n",
    "categorical_features = ['sentences', 'n_words', 'avg_words_sent', 'avg_word_size', 'TTR']\n",
    "\n",
    "X = hstack([csr_matrix(df[categorical_features].values), df_vectorized[0:]])\n",
    "\n",
    "gc.collect();\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 43)\n",
    "\n",
    "########## Model ###########\n",
    "\n",
    "model_rf = RandomForestClassifier(bootstrap = True, max_depth = 95, max_features = 'auto', min_samples_leaf = 1, \n",
    "                        min_samples_split = 4, n_estimators = 1800)\n",
    "\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions & results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.22%\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {round(score * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[120  30]\n",
      " [ 19 123]]\n"
     ]
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred, labels = [0, 1])\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process one new and detect it with the Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    \n",
    "    n_sentences = []\n",
    "    n_words = []\n",
    "    avg_words_sent = []\n",
    "    avg_word_size = []\n",
    "    type_token_ratio = []\n",
    "    \n",
    "    for i, row in corpus.iterrows():\n",
    "        text = df['Text'].iloc[i]\n",
    "\n",
    "        text = text.replace(r\"http\\S+\", \"\")\n",
    "        text = text.replace(r\"http\", \"\")\n",
    "        text = text.replace(r\"@\\S+\", \"\")\n",
    "        text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "        text = text.lower()\n",
    "\n",
    "        sent_tokens = nltk.sent_tokenize(text)\n",
    "\n",
    "        #Number of sentences\n",
    "        number_sentences = len(sent_tokens)\n",
    "\n",
    "        word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        stop_words = stopwords.words('spanish')\n",
    "        stop_words.extend(list(punctuation))\n",
    "        stop_words.extend(['¿', '¡', '\"', '``']) \n",
    "        stop_words.extend(map(str,range(10)))\n",
    "\n",
    "        filtered_tokens = [i for i in word_tokens if i not in stop_words]\n",
    "\n",
    "        #number of tokens\n",
    "        number_words = len(filtered_tokens)\n",
    "\n",
    "        # average words per sentence\n",
    "        avg_word_sentences = (float(number_words)/number_sentences)\n",
    "\n",
    "        # average word size\n",
    "        word_size = sum(len(word) for word in filtered_tokens) / number_words\n",
    "\n",
    "        # type token ratio\n",
    "        types = nltk.Counter(filtered_tokens)\n",
    "        TTR = (len(types) / number_words) * 100\n",
    "\n",
    "        n_sentences.append(number_sentences)\n",
    "        n_words.append(number_words)\n",
    "        avg_words_sent.append(avg_word_sentences)\n",
    "        avg_word_size.append(word_size)\n",
    "        type_token_ratio.append(TTR)\n",
    "\n",
    "    df_features['sentences'] = n_sentences\n",
    "    df_features['n_words'] = n_words\n",
    "    df_features['avg_words_sent'] = avg_words_sent\n",
    "    df_features['avg_word_size'] = avg_word_size\n",
    "    df_features['TTR'] = type_token_ratio\n",
    "    df_features['Text'] = text\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Algunos usuarios de Twitter han empezado a compartir hoy vídeos totalmente insólitos, mostrando elefantes paseando despreocupadamente por carreteras, calles y plazas de diversos puntos del territorio español, donde es muy difícil verlos. “Hasta ahora se ocultaban porque sabían que Juan Carlos I estaba al acecho pero ahora han perdido el miedo y se confían porque saben que no está”, explica. Que [el Rey emérito] vuelva cuanto antes porque no se puede caminar por la calle, hay heces por todas partes y solo han pasado unos días desde que se fue su principal depredador”, se queja en Twitter Álvaro Valdés Fernández, un ciudadano de Madrid que ha pisado un inmenso excremento esta mañana y ya habla de una “auténtica plaga de paquidermos”. La mayoría de españoles, sin embargo, cree que las imágenes son curiosas y es bonito ver las calles, antes desiertas, “llenas de vida y elefantes”. El Gobierno considera que los pequeños inconvenientes que los elefantes puedan ocasionar no pasarán a mayores y, de ser así, recuerdan que Felipe VI está “totalmente preparado” para tomar el relevo de su padre en la cadena trófica y sabe utilizar perfectamente a “Poderoso”, el viejo rifle de cazar elefantes del emérito.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/pipe11/TFM_fake_news_detector/data/corpus_spanish.csv')\n",
    "\n",
    "# Label encoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['Labels'] = labelencoder.fit_transform(df['Category'])\n",
    "y = df['Labels']\n",
    "\n",
    "df_text = pd.DataFrame([[text]], columns = ['Text'])\n",
    "\n",
    "corpus = pd.concat([df['Text'], df_text['Text']], axis = 0)\n",
    "\n",
    "corpus.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features:\n",
    "        \n",
    "    df_features = pd.DataFrame()\n",
    "\n",
    "    n_sentences = []\n",
    "    n_words = []\n",
    "    avg_words_sent = []\n",
    "    avg_word_size = []\n",
    "    type_token_ratio = []\n",
    "    list_text = []\n",
    "\n",
    "    n_corpus = len(corpus)\n",
    "\n",
    "    for i in range(0, n_corpus):\n",
    "        text = corpus.iloc[i]\n",
    "\n",
    "        text = text.replace(r\"http\\S+\", \"\")\n",
    "        text = text.replace(r\"http\", \"\")\n",
    "        text = text.replace(r\"@\\S+\", \"\")\n",
    "        text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "        text = text.lower()\n",
    "\n",
    "        sent_tokens = nltk.sent_tokenize(text)\n",
    "\n",
    "        #Number of sentences\n",
    "        number_sentences = len(sent_tokens)\n",
    "\n",
    "        word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        stop_words = stopwords.words('spanish')\n",
    "        stop_words.extend(list(punctuation))\n",
    "        stop_words.extend(['¿', '¡', '\"', '``']) \n",
    "        stop_words.extend(map(str,range(10)))\n",
    "\n",
    "        filtered_tokens = [i for i in word_tokens if i not in stop_words]\n",
    "\n",
    "        #number of tokens\n",
    "        number_words = len(filtered_tokens)\n",
    "\n",
    "        # average words per sentence\n",
    "        avg_word_sentences = (float(number_words)/number_sentences)\n",
    "\n",
    "        # average word size\n",
    "        word_size = sum(len(word) for word in filtered_tokens) / number_words\n",
    "\n",
    "        # type token ratio\n",
    "        types = nltk.Counter(filtered_tokens)\n",
    "        TTR = (len(types) / number_words) * 100\n",
    "\n",
    "        n_sentences.append(number_sentences)\n",
    "        n_words.append(number_words)\n",
    "        avg_words_sent.append(avg_word_sentences)\n",
    "        avg_word_size.append(word_size)\n",
    "        type_token_ratio.append(TTR)\n",
    "        list_text.append(text)\n",
    "\n",
    "    df_features['sentences'] = n_sentences\n",
    "    df_features['n_words'] = n_words\n",
    "    df_features['avg_words_sent'] = avg_words_sent\n",
    "    df_features['avg_word_size'] = avg_word_size\n",
    "    df_features['TTR'] = type_token_ratio\n",
    "    df_features['Text'] = list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_transformer(corpus):\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer(  \n",
    "                    analyzer = 'word',\n",
    "                    tokenizer = tokenize,\n",
    "                    lowercase = True,\n",
    "                    stop_words = spanish_stopwords)\n",
    "\n",
    "    text_vectorized = tfidf_vectorizer.fit_transform(df_text['Text'])\n",
    "\n",
    "    return text_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine TF-IDF features with Dense features as a dataframe Sparse\n",
    "def feature_combiner(df_features):\n",
    "\n",
    "    categorical_features = ['sentences', 'n_words', 'avg_words_sent', 'avg_word_size', 'TTR']\n",
    "\n",
    "    X = hstack([csr_matrix(df_text[categorical_features].values), text_vectorized[0:]])\n",
    "    text_predict = hstack([csr_matrix(df_text[categorical_features].values), text_vectorized[0:]])\n",
    "    \n",
    "    \n",
    "\n",
    "    gc.collect();\n",
    "\n",
    "    return X_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pipe11/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['algun', 'com', 'contr', 'cuand', 'desd', 'dond', 'durant', 'eram', 'estab', 'estais', 'estam', 'estan', 'estand', 'estaran', 'estaras', 'esteis', 'estem', 'esten', 'estes', 'estuv', 'fuer', 'fues', 'fuim', 'fuist', 'hab', 'habr', 'habran', 'habras', 'hast', 'hem', 'hub', 'mas', 'mia', 'mias', 'mio', 'mios', 'much', 'nad', 'nosotr', 'nuestr', 'par', 'per', 'poc', 'porqu', 'qui', 'seais', 'seam', 'sent', 'ser', 'seran', 'seras', 'si', 'sient', 'sint', 'sobr', 'som', 'suy', 'tambien', 'tant', 'ten', 'tendr', 'tendran', 'tendras', 'teng', 'tien', 'tod', 'tuv', 'tuy', 'vosotr', 'vuestr'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 15569 and input n_features is 106 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-566c213154f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_combiner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \"\"\"\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    397\u001b[0m                              \u001b[0;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                              \u001b[0;34m\"input n_features is %s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 15569 and input n_features is 106 "
     ]
    }
   ],
   "source": [
    "# def fake_news_detector(text):\n",
    "df_text = extract_features(text)\n",
    "text_vectorized = tfidf_transformer(df_text)\n",
    "X_text = feature_combiner(df_text)\n",
    "model_rf.predict(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-eecfd20fdc80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_text' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf_transformer(df_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
