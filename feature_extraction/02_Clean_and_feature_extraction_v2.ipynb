{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and feature extraction v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text, extract stylometric features and create a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/corpus_spanish.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>641</td>\n",
       "      <td>True</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>Caras</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>Sofía Castro y Alejandro Peña Pretelini: una i...</td>\n",
       "      <td>https://www.caras.com.mx/sofia-castro-alejandr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>Education</td>\n",
       "      <td>Heraldo</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'online'</td>\n",
       "      <td>Un paso más cerca de hacer los exámenes 'onlin...</td>\n",
       "      <td>https://www.heraldo.es/noticias/suplementos/he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141</td>\n",
       "      <td>True</td>\n",
       "      <td>Science</td>\n",
       "      <td>HUFFPOST</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>Esto es lo que los científicos realmente piens...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/scientist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>394</td>\n",
       "      <td>True</td>\n",
       "      <td>Politics</td>\n",
       "      <td>El financiero</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>Inicia impresión de boletas para elección pres...</td>\n",
       "      <td>http://www.elfinanciero.com.mx/elecciones-2018...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139</td>\n",
       "      <td>True</td>\n",
       "      <td>Sport</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>A *NUMBER* día del Mundial</td>\n",
       "      <td>A *NUMBER* día del Mundial\\nFIFA.com sigue la ...</td>\n",
       "      <td>https://es.fifa.com/worldcup/news/a-1-dia-del-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id Category          Topic         Source  \\\n",
       "0  641     True  Entertainment          Caras   \n",
       "1    6     True      Education        Heraldo   \n",
       "2  141     True        Science       HUFFPOST   \n",
       "3  394     True       Politics  El financiero   \n",
       "4  139     True          Sport           FIFA   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1   Un paso más cerca de hacer los exámenes 'online'   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4                         A *NUMBER* día del Mundial   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Sofía Castro y Alejandro Peña Pretelini: una i...   \n",
       "1  Un paso más cerca de hacer los exámenes 'onlin...   \n",
       "2  Esto es lo que los científicos realmente piens...   \n",
       "3  Inicia impresión de boletas para elección pres...   \n",
       "4  A *NUMBER* día del Mundial\\nFIFA.com sigue la ...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.caras.com.mx/sofia-castro-alejandr...  \n",
       "1  https://www.heraldo.es/noticias/suplementos/he...  \n",
       "2  https://www.huffingtonpost.com/entry/scientist...  \n",
       "3  http://www.elfinanciero.com.mx/elecciones-2018...  \n",
       "4  https://es.fifa.com/worldcup/news/a-1-dia-del-...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(971, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id           int64\n",
       "Category    object\n",
       "Topic       object\n",
       "Source      object\n",
       "Headline    object\n",
       "Text        object\n",
       "Link        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 971 entries, 0 to 970\n",
      "Data columns (total 7 columns):\n",
      "Id          971 non-null int64\n",
      "Category    971 non-null object\n",
      "Topic       971 non-null object\n",
      "Source      971 non-null object\n",
      "Headline    971 non-null object\n",
      "Text        971 non-null object\n",
      "Link        971 non-null object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 53.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and complexity features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "We are using this encoding technique for the target label instead one hot encoding, reasons:\n",
    "\n",
    " - The categorical features are binary\n",
    " - Not problem with features being ordinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.5 s, sys: 281 ms, total: 5.78 s\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk import word_tokenize, sent_tokenize  \n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['Label'] = labelencoder.fit_transform(df['Category'])\n",
    "\n",
    "df_features = pd.DataFrame()\n",
    "\n",
    "list_text = []\n",
    "list_headline = []\n",
    "list_sentences_t = []\n",
    "list_words_t = []\n",
    "list_words_sent_t = []\n",
    "list_word_size_t = []\n",
    "list_ttr_t = []\n",
    "list_sentences_h = []\n",
    "list_words_h = []\n",
    "list_words_sent_h = []\n",
    "list_word_size_h = []\n",
    "list_ttr_h = []\n",
    "\n",
    "for n, row in df.iterrows():\n",
    "    \n",
    "    headline = df['Headline'].iloc[n]\n",
    "    text = df['Text'].iloc[n]\n",
    "    \n",
    "    text = text.replace(r\"http\\S+\", \"\")\n",
    "    text = text.replace(r\"http\", \"\")\n",
    "    text = text.replace(r\"@\\S+\", \"\")\n",
    "    text = text.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    text = text.lower()\n",
    "        \n",
    "    headline = headline.replace(r\"http\\S+\", \"\")\n",
    "    headline = headline.replace(r\"http\", \"\")\n",
    "    headline = headline.replace(r\"@\\S+\", \"\")\n",
    "    headline = headline.replace(r\"(?<!\\n)\\n(?!\\n)\", \" \")\n",
    "    headline = headline.lower()\n",
    "\n",
    "    sent_tokens_text = nltk.sent_tokenize(text)\n",
    "    sent_tokens_headline = nltk.sent_tokenize(headline)\n",
    "\n",
    "    # number of sentences\n",
    "    n_sentences_text = len(sent_tokens_text)\n",
    "    n_sentences_headline = len(sent_tokens_headline)\n",
    "\n",
    "    word_tokens_text = nltk.word_tokenize(text)\n",
    "    word_tokens_headline = nltk.word_tokenize(headline)\n",
    "\n",
    "    stop_words = stopwords.words('spanish')\n",
    "    stop_words.extend(list(punctuation))\n",
    "    stop_words.extend(['¿', '¡', '\"', '``']) \n",
    "    stop_words.extend(map(str,range(10)))\n",
    "\n",
    "    filtered_tokens_text = [n for n in word_tokens_text if n not in stop_words]\n",
    "    filtered_tokens_headline = [n for n in word_tokens_headline if n not in stop_words]\n",
    "\n",
    "    # number of tokens/words\n",
    "    n_words_text = len(filtered_tokens_text)\n",
    "    n_words_headline = len(filtered_tokens_headline)\n",
    "\n",
    "    # average words per sentence\n",
    "    avg_word_sentences_text = (float(n_words_text) / n_sentences_text)\n",
    "#     avg_word_sentences_headline = (float(n_words_headline) / n_sentences_headline)\n",
    "\n",
    "    # average word size\n",
    "    word_size_text = sum(len(word) for word in filtered_tokens_text) / n_words_text\n",
    "    word_size_headline = sum(len(word) for word in filtered_tokens_headline) / n_words_headline\n",
    "\n",
    "    # type token ratio\n",
    "    types_text = nltk.Counter(filtered_tokens_text)\n",
    "    ttr_text = (len(types_text) / n_words_text) * 100\n",
    "    \n",
    "    types_headline = nltk.Counter(filtered_tokens_headline)\n",
    "    ttr_headline = (len(types_headline) / n_words_headline) * 100\n",
    "    \n",
    "    # text\n",
    "    list_text.append(text)\n",
    "    list_sentences_t.append(n_sentences_text)\n",
    "    list_words_t.append(n_words_text)\n",
    "    list_words_sent_t.append(avg_word_sentences_text)\n",
    "    list_word_size_t.append(word_size_text)\n",
    "    list_ttr_t.append(ttr_text)\n",
    "    \n",
    "    # headline\n",
    "    list_headline.append(headline)    \n",
    "#     list_sentences_h.append(n_sentences_headline) #irrelevant\n",
    "    list_words_h.append(n_words_headline)\n",
    "#     list_words_sent_h.append(avg_word_sentences_headline) # irrelevant\n",
    "    list_word_size_h.append(word_size_headline)\n",
    "    list_ttr_h.append(ttr_headline)\n",
    "\n",
    "df_features['headline'] = list_headline\n",
    "df_features['text'] = list_text\n",
    "df_features['n_sentences_text'] = list_sentences_t\n",
    "df_features['n_words_text'] = list_words_t\n",
    "df_features['avg_words_sent_text'] = list_words_sent_t\n",
    "df_features['avg_word_size_text'] = list_word_size_t\n",
    "df_features['ttr_text'] = list_ttr_t\n",
    "# df_features['n_sentences_headline'] # list_sentences_h # irrelevant\n",
    "df_features['n_words_headline'] = list_words_h\n",
    "# df_features['avg_words_sent_headline'] = list_words_sent_h # irrelevant\n",
    "df_features['avg_word_size_headline'] = list_word_size_h\n",
    "df_features['ttr_headline'] = list_ttr_h\n",
    "df_features['label'] = df['Label']\n",
    "\n",
    "df_features.to_csv('../data/spanish_corpus_features_v2.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>n_sentences_text</th>\n",
       "      <th>n_words_text</th>\n",
       "      <th>avg_words_sent_text</th>\n",
       "      <th>avg_word_size_text</th>\n",
       "      <th>ttr_text</th>\n",
       "      <th>n_words_headline</th>\n",
       "      <th>avg_word_size_headline</th>\n",
       "      <th>ttr_headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sofía castro y alejandro peña pretelini: una i...</td>\n",
       "      <td>sofía castro y alejandro peña pretelini: una i...</td>\n",
       "      <td>5</td>\n",
       "      <td>123</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>6.398374</td>\n",
       "      <td>69.105691</td>\n",
       "      <td>8</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>un paso más cerca de hacer los exámenes 'online'</td>\n",
       "      <td>un paso más cerca de hacer los exámenes 'onlin...</td>\n",
       "      <td>8</td>\n",
       "      <td>224</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>7.205357</td>\n",
       "      <td>77.232143</td>\n",
       "      <td>5</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>esto es lo que los científicos realmente piens...</td>\n",
       "      <td>esto es lo que los científicos realmente piens...</td>\n",
       "      <td>29</td>\n",
       "      <td>467</td>\n",
       "      <td>16.103448</td>\n",
       "      <td>7.573876</td>\n",
       "      <td>64.668094</td>\n",
       "      <td>4</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inicia impresión de boletas para elección pres...</td>\n",
       "      <td>inicia impresión de boletas para elección pres...</td>\n",
       "      <td>10</td>\n",
       "      <td>167</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>7.964072</td>\n",
       "      <td>63.473054</td>\n",
       "      <td>5</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a *number* día del mundial</td>\n",
       "      <td>a *number* día del mundial\\nfifa.com sigue la ...</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>7.368421</td>\n",
       "      <td>84.210526</td>\n",
       "      <td>3</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interpol ordena detención inmediata de osorio ...</td>\n",
       "      <td>interpol ordena detención inmediata de osorio ...</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>18.666667</td>\n",
       "      <td>7.732143</td>\n",
       "      <td>89.285714</td>\n",
       "      <td>8</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"los ninis\" más ricos y poderosos del país: hi...</td>\n",
       "      <td>\"los ninis\" más ricos y poderosos del país: hi...</td>\n",
       "      <td>5</td>\n",
       "      <td>81</td>\n",
       "      <td>16.200000</td>\n",
       "      <td>6.358025</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>7</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gobierno de alfredo del mazo inició con récord...</td>\n",
       "      <td>para todo sacan lo del populismo, ni siquiera ...</td>\n",
       "      <td>11</td>\n",
       "      <td>183</td>\n",
       "      <td>16.636364</td>\n",
       "      <td>6.677596</td>\n",
       "      <td>79.234973</td>\n",
       "      <td>6</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conapred investiga acto de racismo en el pumas...</td>\n",
       "      <td>conapred investiga acto de racismo en el pumas...</td>\n",
       "      <td>6</td>\n",
       "      <td>105</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>6.419048</td>\n",
       "      <td>74.285714</td>\n",
       "      <td>7</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cristiano ronaldo acepta dos años de prisión</td>\n",
       "      <td>cristiano ronaldo acepta dos años de prisión\\n...</td>\n",
       "      <td>16</td>\n",
       "      <td>270</td>\n",
       "      <td>16.875000</td>\n",
       "      <td>6.918519</td>\n",
       "      <td>59.259259</td>\n",
       "      <td>6</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  sofía castro y alejandro peña pretelini: una i...   \n",
       "1   un paso más cerca de hacer los exámenes 'online'   \n",
       "2  esto es lo que los científicos realmente piens...   \n",
       "3  inicia impresión de boletas para elección pres...   \n",
       "4                         a *number* día del mundial   \n",
       "5  interpol ordena detención inmediata de osorio ...   \n",
       "6  \"los ninis\" más ricos y poderosos del país: hi...   \n",
       "7  gobierno de alfredo del mazo inició con récord...   \n",
       "8  conapred investiga acto de racismo en el pumas...   \n",
       "9       cristiano ronaldo acepta dos años de prisión   \n",
       "\n",
       "                                                text  n_sentences_text  \\\n",
       "0  sofía castro y alejandro peña pretelini: una i...                 5   \n",
       "1  un paso más cerca de hacer los exámenes 'onlin...                 8   \n",
       "2  esto es lo que los científicos realmente piens...                29   \n",
       "3  inicia impresión de boletas para elección pres...                10   \n",
       "4  a *number* día del mundial\\nfifa.com sigue la ...                 4   \n",
       "5  interpol ordena detención inmediata de osorio ...                 3   \n",
       "6  \"los ninis\" más ricos y poderosos del país: hi...                 5   \n",
       "7  para todo sacan lo del populismo, ni siquiera ...                11   \n",
       "8  conapred investiga acto de racismo en el pumas...                 6   \n",
       "9  cristiano ronaldo acepta dos años de prisión\\n...                16   \n",
       "\n",
       "   n_words_text  avg_words_sent_text  avg_word_size_text   ttr_text  \\\n",
       "0           123            24.600000            6.398374  69.105691   \n",
       "1           224            28.000000            7.205357  77.232143   \n",
       "2           467            16.103448            7.573876  64.668094   \n",
       "3           167            16.700000            7.964072  63.473054   \n",
       "4            57            14.250000            7.368421  84.210526   \n",
       "5            56            18.666667            7.732143  89.285714   \n",
       "6            81            16.200000            6.358025  81.481481   \n",
       "7           183            16.636364            6.677596  79.234973   \n",
       "8           105            17.500000            6.419048  74.285714   \n",
       "9           270            16.875000            6.918519  59.259259   \n",
       "\n",
       "   n_words_headline  avg_word_size_headline  ttr_headline  label  \n",
       "0                 8                7.500000         100.0      1  \n",
       "1                 5                5.800000         100.0      1  \n",
       "2                 4                9.500000         100.0      1  \n",
       "3                 5                8.400000         100.0      1  \n",
       "4                 3                5.333333         100.0      1  \n",
       "5                 8                7.750000         100.0      0  \n",
       "6                 7                4.857143         100.0      0  \n",
       "7                 6                6.500000         100.0      1  \n",
       "8                 7                6.000000         100.0      1  \n",
       "9                 6                6.000000         100.0      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting because the type token ratio seems to be 100 in every headline, except this 31 headlines:\n",
    "Looking at them we realize there are the double type token ration less than 100 in fake news than the real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>n_sentences_text</th>\n",
       "      <th>n_words_text</th>\n",
       "      <th>avg_words_sent_text</th>\n",
       "      <th>avg_word_size_text</th>\n",
       "      <th>ttr_text</th>\n",
       "      <th>n_words_headline</th>\n",
       "      <th>avg_word_size_headline</th>\n",
       "      <th>ttr_headline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       headline  text  n_sentences_text  n_words_text  avg_words_sent_text  \\\n",
       "label                                                                        \n",
       "0            20    20                20            20                   20   \n",
       "1            11    11                11            11                   11   \n",
       "\n",
       "       avg_word_size_text  ttr_text  n_words_headline  avg_word_size_headline  \\\n",
       "label                                                                           \n",
       "0                      20        20                20                      20   \n",
       "1                      11        11                11                      11   \n",
       "\n",
       "       ttr_headline  \n",
       "label                \n",
       "0                20  \n",
       "1                11  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features[df_features['ttr_headline'] < 100.0].groupby('label').count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
