{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "This is the final notebook explaining the methodology for text feature extraction. We will explain each of the different features extracted and then we will explain the process and methodology to extract all the features and create a new dataset.\n",
    "\n",
    "## Index\n",
    "\n",
    "- [1. Features](#1.-Feature-explanation)\n",
    "\n",
    " - [1.1. Complexity features](#1.1.-Complexity-features)\n",
    " - [1.2. Stylometric features](#1.2.-Stylometric-features)\n",
    "\n",
    "\n",
    "- [2. Requisites](#2.-Requisites)\n",
    "\n",
    "\n",
    "- [3. Feature extraction for training](#3.-Feature-extraction-for-training)\n",
    "\n",
    "\n",
    "- [4. Feature extraction function for predictions](#4.-Feature-extraction-function-for-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature explanation\n",
    "\n",
    "On this section we will explain the features that we are going to extract from the News Headline and News Content text.These features are language-independent, for example, they do not consider specific terms from a language, in this case spanish.\n",
    "\n",
    "Our objective is to extract features based on high-level structures. To accomplish this objective, we are going to extract features from 2 categories: Complexity and Stylometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Complexity features\n",
    "\n",
    "The objective of these features is to capture te overall intricacy of the news, in sentence and word level. To achive this, we use metrics like average word size, words count per sentence and type token ratio:\n",
    "\n",
    "**words**: Number of words\n",
    "\n",
    "**Sents**: Number of sentences\n",
    "\n",
    "**avg_words_sentence**: Average words per sentence\n",
    "\n",
    "**avg_word_size**: Average word size\n",
    "\n",
    "**avg_syllables_word**: Average syllables per word\n",
    "\n",
    "**unique_words**: Hapaxes or unique words that only appears once in a text\n",
    "\n",
    "**ttr**: Type token ratio\n",
    "\n",
    "### Bonus ###\n",
    "\n",
    "Spanish readability tests:\n",
    "\n",
    "**huerta_score**: Fernández Huerta's redability score (Reading comprehension of the text), spanish adaptation of the Flesch equation\n",
    "\n",
    "&nbsp;&nbsp; $$Readability = 206.84 - 0.60 \\times Average Syllables Word - 1.02 \\times Average Words Sentence$$ &nbsp;&nbsp;\n",
    "\n",
    "**szigriszt_score**: Szigriszt Pazos perspicuity score (Legibility and clarity of the text), a modern spanish adaptation of the Flesch equation.\n",
    "\n",
    "&nbsp;&nbsp; $$Perspicuity = 206.835 - \\frac{62.3 \\times TotalSyllables}{Words} - \\frac{Words}{Sentences}$$ &nbsp;&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Stylometric features\n",
    "For stylometric or lexical features, we use NLP techniques to extract grammatical and lexical information for each text. We are using Spacy POS tagging techniques to track different word style frequencies:\n",
    "\n",
    "**mltd**: Measure of Textual Lexical Diversity, based on McCarthy and Jarvis (2010).\n",
    "\n",
    "**upper_case_ratio**: Uppercase letters to all letters ratio\n",
    "\n",
    "**entityratio**: Ratio of named Entities to the text size\n",
    "\n",
    "**quotes**: Number of quotes\n",
    "\n",
    "**quotes_ratio**: Ratio of quotes marks to text size\n",
    "\n",
    "**propn_ratio**: Proper Noun tag frequency\n",
    "\n",
    "**noun_ratio**: Noun tag frequency\n",
    "\n",
    "**pron_ratio**: Pronoun tag frequency\n",
    "\n",
    "**adp_ratio**: Adposition tag frequency\n",
    "\n",
    "**det_ratio**: Determinant tag frequency\n",
    "\n",
    "**punct_ratio**: Punctuation tag frequency\n",
    "\n",
    "**verb_ratio**: Verb tag frequency\n",
    "\n",
    "**adv_ratio**: Adverb tag frequency\n",
    "\n",
    "**sym_ratio**: Symbol tag frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Requisites\n",
    "\n",
    "*For Python 3 installations use ___!pip3 install___ and ___python3 *___\n",
    "\n",
    "[NLTK package](https://pypi.org/project/nltk/)\n",
    "\n",
    "`!pip install nltk`\n",
    "\n",
    "`import nltk`\n",
    "\n",
    "\n",
    "[Spacy spanish package](https://spacy.io/models/es)\n",
    "\n",
    "`!pip install spacy`\n",
    "\n",
    "`python -m spacy download es_core_news_lg`\n",
    "\n",
    "`import spacy`\n",
    "\n",
    "\n",
    "[lexical_diversity package](https://pypi.org/project/lexical-diversity/)\n",
    "\n",
    "`!pip install lexical-diversity`\n",
    "\n",
    "`from lexical_diversity import lex_div as ld`\n",
    "\n",
    "\n",
    "[Syltippy](https://github.com/nur-ag/syltippy)\n",
    "\n",
    "Syltippy is a simple, user friendly word syllabization package for spanish language with no additional dependencies.\n",
    "\n",
    "`!pip install syltippy`\n",
    "\n",
    "`from syltippy import syllabize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature extraction for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried several syllabizers for spanish and this is the chosen solution. Believe me, i spent a whole day.\n",
    "# I had to replace all symbols, punctuations and it includes accentuation from other languages like ä, à, etc...\n",
    "# It's a bit inconsistent with words from others languages, acronyms and abreviations. However it performs really well for our case!!!\n",
    "\n",
    "def get_nsyllables(text):\n",
    "    from syltippy import syllabize\n",
    "\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub(r'[^ \\nA-Za-z0-9ÁÉÍÓÚÑáéíóúñ/]+', '', text)\n",
    "    \n",
    "    n_syllables = len(syllabize(text)[0])\n",
    "    \n",
    "    return n_syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 21s, sys: 3.12 s, total: 5min 24s\n",
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lexical_diversity import lex_div as ld\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "df = pd.read_csv('../data/corpus_spanish_v3.csv')\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['Label'] = labelencoder.fit_transform(df['Category'])\n",
    "\n",
    "# empty lists and df\n",
    "df_features = pd.DataFrame()\n",
    "list_text = []\n",
    "list_sentences = []\n",
    "list_words = []\n",
    "list_words_sent = []\n",
    "list_word_size = []\n",
    "list_avg_syllables_word = []\n",
    "list_unique_words = []\n",
    "list_ttr = []\n",
    "list_huerta_score = []\n",
    "list_szigriszt_score = []\n",
    "list_mltd = []\n",
    "list_entity_ratio = []\n",
    "list_upper_case_ratio = []\n",
    "list_quotes = []\n",
    "list_quotes_ratio = []\n",
    "list_propn_ratio = [] \n",
    "list_noun_ratio = []\n",
    "list_adp_ratio = []\n",
    "list_det_ratio = []\n",
    "list_punct_ratio = []\n",
    "list_pron_ratio = []\n",
    "list_verb_ratio = []\n",
    "list_adv_ratio = []\n",
    "list_sym_ratio = []\n",
    "\n",
    "list_headline = []\n",
    "list_words_h = []\n",
    "list_word_size_h = []\n",
    "list_avg_syllables_word_h = []\n",
    "list_ttr_h = []\n",
    "list_mltd_h = []\n",
    "list_unique_words_h = []\n",
    "\n",
    "# df iteration\n",
    "for n, row in df.iterrows():\n",
    "    \n",
    "    ## headline ##\n",
    "    headline = df['Headline'].iloc[n]\n",
    "    headline = re.sub(r\"http\\S+\", \"\", headline)\n",
    "    headline = re.sub(r\"http\", \"\", headline)\n",
    "    headline = re.sub(r\"@\\S+\", \"\", headline)\n",
    "    headline = re.sub(\"\\n\", \" \", headline)\n",
    "    headline = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", headline)\n",
    "    headline = headline.replace(r\"*NUMBER*\", \"número\")\n",
    "    headline = headline.replace(r\"*PHONE*\", \"número\")\n",
    "    headline = headline.replace(r\"*EMAIL*\", \"email\")\n",
    "    headline = headline.replace(r\"*URL*\", \"url\")\n",
    "    headline = headline.lower()\n",
    "    doc_h = nlp(headline)\n",
    "    \n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "    syllables_h = get_nsyllables(headline)\n",
    "    words_h = len(list_tokens_h)\n",
    "    \n",
    "    # headline complexity features\n",
    "    avg_word_size_h = round(sum(len(word) for word in list_tokens_h) / words_h, 2)\n",
    "    avg_syllables_word_h = round(syllables_h / words_h, 2)\n",
    "    unique_words_h = round((len(fdist_h.hapaxes()) / words_h) * 100, 2)\n",
    "    ttr_h = round(ld.ttr(list_tokens_h) * 100, 2)\n",
    "    mltd_h = round(ld.mtld(list_tokens_h), 2)\n",
    "    \n",
    "    ## text content##   \n",
    "    text = df['Text'].iloc[n]  \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"http\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "    \n",
    "    # to later calculate upper case letters ratio\n",
    "    alph = list(filter(str.isalpha, text))\n",
    "    text_lower = text.lower()\n",
    "    doc = nlp(text_lower)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    list_entities = []\n",
    "    sents = 0\n",
    "    \n",
    "    for entity in doc.ents:\n",
    "        list_entities.append(entity.label_)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "    \n",
    "    # Calculate entities, pos, tag, freq, syllables, words and quotes\n",
    "    entities = len(list_entities)\n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "    syllables = get_nsyllables(text)\n",
    "    words = len(list_tokens)\n",
    "    quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "\n",
    "    # complexity features\n",
    "    avg_word_sentence = round(words / sents, 2)\n",
    "    avg_word_size = round(sum(len(word) for word in list_tokens) / words, 2)\n",
    "    avg_syllables_word = round(syllables / words, 2)\n",
    "    unique_words = round((len(fdist.hapaxes()) / words) * 100, 2)\n",
    "    ttr = round(ld.ttr(list_tokens) * 100, 2)\n",
    "    mltd = round(ld.mtld(list_tokens), 2)\n",
    "\n",
    "    # readability spanish test\n",
    "    huerta_score = round(206.84 - (60 * avg_syllables_word) - (1.02 * avg_word_sentence), 2)\n",
    "    szigriszt_score = round(206.835 - ((62.3 * syllables) / words) - (words / sents), 2)\n",
    "\n",
    "    # stylometric features\n",
    "    upper_case_ratio = round(sum(map(str.isupper, alph)) / len(alph) * 100, 2)\n",
    "    entity_ratio = round((entities / words) * 100, 2)\n",
    "    quotes_ratio = round((quotes / words) * 100, 2)\n",
    "    propn_ratio = round((n_pos['PROPN'] / words) * 100 , 2)\n",
    "    noun_ratio = round((n_pos['NOUN'] / words) * 100, 2) \n",
    "    adp_ratio = round((n_pos['ADP'] / words) * 100, 2)\n",
    "    det_ratio = round((n_pos['DET'] / words) * 100, 2)\n",
    "    punct_ratio = round((n_pos['PUNCT'] / words) * 100, 2)\n",
    "    pron_ratio = round((n_pos['PRON'] / words) * 100, 2)\n",
    "    verb_ratio = round((n_pos['VERB'] / words) * 100, 2)\n",
    "    adv_ratio = round((n_pos['ADV'] / words) * 100, 2)\n",
    "    sym_ratio = round((n_tag['SYM'] / words) * 100, 2)\n",
    "    \n",
    "    # appending on lists\n",
    "    # headline\n",
    "    list_headline.append(headline)\n",
    "    list_words_h.append(words_h)\n",
    "    list_word_size_h.append(avg_word_size_h)\n",
    "    list_avg_syllables_word_h.append(avg_syllables_word_h)\n",
    "    list_unique_words_h.append(unique_words_h)\n",
    "    list_ttr_h.append(ttr_h)\n",
    "    list_mltd_h.append(mltd_h)\n",
    "    \n",
    "    # text\n",
    "    list_text.append(text_lower)\n",
    "    list_sentences.append(sents)\n",
    "    list_words.append(words)\n",
    "    list_words_sent.append(avg_word_sentence)\n",
    "    list_word_size.append(avg_word_size)\n",
    "    list_avg_syllables_word.append(avg_syllables_word)\n",
    "    list_unique_words.append(unique_words)\n",
    "    list_ttr.append(ttr)\n",
    "    list_huerta_score.append(huerta_score)\n",
    "    list_szigriszt_score.append(szigriszt_score)\n",
    "    list_mltd.append(mltd)\n",
    "    list_entity_ratio.append(entity_ratio)\n",
    "    list_upper_case_ratio.append(upper_case_ratio)\n",
    "    list_quotes.append(quotes)\n",
    "    list_quotes_ratio.append(quotes_ratio)\n",
    "    list_propn_ratio.append(propn_ratio)\n",
    "    list_noun_ratio.append(noun_ratio)\n",
    "    list_adp_ratio.append(adp_ratio)\n",
    "    list_det_ratio.append(det_ratio)\n",
    "    list_punct_ratio.append(punct_ratio)\n",
    "    list_pron_ratio.append(pron_ratio)\n",
    "    list_verb_ratio.append(verb_ratio)\n",
    "    list_adv_ratio.append(adv_ratio)\n",
    "    list_sym_ratio.append(sym_ratio)\n",
    "    \n",
    "# dataframe\n",
    "df_features['topic'] = df['Topic']\n",
    "df_features['text'] = list_text\n",
    "df_features['headline'] = list_headline\n",
    "\n",
    "# headline\n",
    "df_features['words_h'] = list_words_h\n",
    "df_features['word_size_h'] = list_word_size_h\n",
    "df_features['avg_syllables_word_h'] = list_avg_syllables_word_h\n",
    "df_features['unique_words_h'] = list_unique_words_h\n",
    "df_features['ttr_h'] = list_ttr_h\n",
    "df_features['mltd_h'] = list_mltd_h\n",
    "\n",
    "# text\n",
    "df_features['sents'] = list_sentences\n",
    "df_features['words'] = list_words\n",
    "df_features['avg_words_sent'] = list_words_sent\n",
    "df_features['avg_word_size'] = list_word_size\n",
    "df_features['avg_syllables_word'] = list_avg_syllables_word\n",
    "df_features['unique_words'] = list_unique_words\n",
    "df_features['ttr'] = list_ttr\n",
    "df_features['mltd'] = list_mltd\n",
    "df_features['huerta_score'] = list_huerta_score\n",
    "df_features['szigriszt_score'] = list_szigriszt_score\n",
    "df_features['upper_case_ratio'] = list_upper_case_ratio\n",
    "df_features['entity_ratio'] = list_entity_ratio\n",
    "df_features['quotes'] = list_quotes\n",
    "df_features['quotes_ratio'] = list_quotes_ratio\n",
    "df_features['propn_ratio'] = list_propn_ratio\n",
    "df_features['noun_ratio'] = list_noun_ratio\n",
    "df_features['adp_ratio'] = list_adp_ratio\n",
    "df_features['det_ratio'] = list_det_ratio\n",
    "df_features['punct_ratio'] = list_punct_ratio\n",
    "df_features['pron_ratio'] = list_pron_ratio\n",
    "df_features['verb_ratio'] = list_verb_ratio\n",
    "df_features['adv_ratio'] = list_adv_ratio\n",
    "df_features['sym_ratio'] = list_sym_ratio\n",
    "\n",
    "df_features['label'] = df['Label']\n",
    "\n",
    "df_features.to_csv('../data/spanish_corpus_features_v6.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>text</th>\n",
       "      <th>headline</th>\n",
       "      <th>words_h</th>\n",
       "      <th>word_size_h</th>\n",
       "      <th>avg_syllables_word_h</th>\n",
       "      <th>unique_words_h</th>\n",
       "      <th>ttr_h</th>\n",
       "      <th>mltd_h</th>\n",
       "      <th>sents</th>\n",
       "      <th>words</th>\n",
       "      <th>avg_words_sent</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>avg_syllables_word</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>ttr</th>\n",
       "      <th>mltd</th>\n",
       "      <th>huerta_score</th>\n",
       "      <th>szigriszt_score</th>\n",
       "      <th>upper_case_ratio</th>\n",
       "      <th>entity_ratio</th>\n",
       "      <th>quotes</th>\n",
       "      <th>quotes_ratio</th>\n",
       "      <th>propn_ratio</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>adp_ratio</th>\n",
       "      <th>det_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>pron_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adv_ratio</th>\n",
       "      <th>sym_ratio</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Science</td>\n",
       "      <td>la nasa recupera el contacto con un satélite d...</td>\n",
       "      <td>la nasa recupera el contacto con un satélite d...</td>\n",
       "      <td>16</td>\n",
       "      <td>5.38</td>\n",
       "      <td>2.50</td>\n",
       "      <td>87.50</td>\n",
       "      <td>93.75</td>\n",
       "      <td>71.68</td>\n",
       "      <td>16</td>\n",
       "      <td>479</td>\n",
       "      <td>29.94</td>\n",
       "      <td>4.76</td>\n",
       "      <td>2.04</td>\n",
       "      <td>31.52</td>\n",
       "      <td>43.22</td>\n",
       "      <td>56.66</td>\n",
       "      <td>53.90</td>\n",
       "      <td>49.96</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.80</td>\n",
       "      <td>24.43</td>\n",
       "      <td>17.75</td>\n",
       "      <td>15.66</td>\n",
       "      <td>7.10</td>\n",
       "      <td>1.67</td>\n",
       "      <td>8.35</td>\n",
       "      <td>3.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Economy</td>\n",
       "      <td>amlo aceleraría el consumo y el crecimiento ec...</td>\n",
       "      <td>amlo aceleraría el consumo y el crecimiento ec...</td>\n",
       "      <td>11</td>\n",
       "      <td>5.27</td>\n",
       "      <td>2.55</td>\n",
       "      <td>81.82</td>\n",
       "      <td>90.91</td>\n",
       "      <td>33.88</td>\n",
       "      <td>5</td>\n",
       "      <td>206</td>\n",
       "      <td>41.20</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.89</td>\n",
       "      <td>33.98</td>\n",
       "      <td>49.03</td>\n",
       "      <td>57.83</td>\n",
       "      <td>51.42</td>\n",
       "      <td>47.69</td>\n",
       "      <td>4.35</td>\n",
       "      <td>5.83</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97</td>\n",
       "      <td>9.22</td>\n",
       "      <td>22.82</td>\n",
       "      <td>15.53</td>\n",
       "      <td>16.50</td>\n",
       "      <td>11.17</td>\n",
       "      <td>0.49</td>\n",
       "      <td>6.31</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sport</td>\n",
       "      <td>al borde de un colapso nervioso quedó el hábil...</td>\n",
       "      <td>compañero de james se ‘calvea’ y le juega pesa...</td>\n",
       "      <td>12</td>\n",
       "      <td>3.75</td>\n",
       "      <td>1.67</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13</td>\n",
       "      <td>368</td>\n",
       "      <td>28.31</td>\n",
       "      <td>4.23</td>\n",
       "      <td>1.70</td>\n",
       "      <td>41.58</td>\n",
       "      <td>53.26</td>\n",
       "      <td>64.43</td>\n",
       "      <td>75.96</td>\n",
       "      <td>72.72</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3.80</td>\n",
       "      <td>7</td>\n",
       "      <td>1.90</td>\n",
       "      <td>7.34</td>\n",
       "      <td>16.58</td>\n",
       "      <td>15.22</td>\n",
       "      <td>13.32</td>\n",
       "      <td>13.04</td>\n",
       "      <td>4.35</td>\n",
       "      <td>8.15</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Politics</td>\n",
       "      <td>mediante pupitrazo de último minuto anoche, el...</td>\n",
       "      <td>dian gravará este año a los niños que recojan ...</td>\n",
       "      <td>12</td>\n",
       "      <td>4.58</td>\n",
       "      <td>1.92</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "      <td>269</td>\n",
       "      <td>33.62</td>\n",
       "      <td>4.75</td>\n",
       "      <td>1.92</td>\n",
       "      <td>40.15</td>\n",
       "      <td>53.53</td>\n",
       "      <td>65.16</td>\n",
       "      <td>57.35</td>\n",
       "      <td>53.71</td>\n",
       "      <td>2.02</td>\n",
       "      <td>1.49</td>\n",
       "      <td>4</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.23</td>\n",
       "      <td>22.30</td>\n",
       "      <td>19.33</td>\n",
       "      <td>15.24</td>\n",
       "      <td>10.04</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6.69</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Politics</td>\n",
       "      <td>muy temprano esta mañana el expresidente y aho...</td>\n",
       "      <td>uribe asegura que insultó 358 guerrilleros</td>\n",
       "      <td>6</td>\n",
       "      <td>6.17</td>\n",
       "      <td>2.50</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>260</td>\n",
       "      <td>28.89</td>\n",
       "      <td>4.18</td>\n",
       "      <td>1.59</td>\n",
       "      <td>49.62</td>\n",
       "      <td>61.54</td>\n",
       "      <td>93.74</td>\n",
       "      <td>81.97</td>\n",
       "      <td>78.75</td>\n",
       "      <td>1.81</td>\n",
       "      <td>2.69</td>\n",
       "      <td>7</td>\n",
       "      <td>2.69</td>\n",
       "      <td>5.77</td>\n",
       "      <td>17.31</td>\n",
       "      <td>8.46</td>\n",
       "      <td>10.77</td>\n",
       "      <td>20.00</td>\n",
       "      <td>5.38</td>\n",
       "      <td>10.38</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3969</th>\n",
       "      <td>Sport</td>\n",
       "      <td>fifa levanta sanción a messi para regresar a l...</td>\n",
       "      <td>fifa levanta sanción a messi para regresar a l...</td>\n",
       "      <td>10</td>\n",
       "      <td>5.10</td>\n",
       "      <td>2.30</td>\n",
       "      <td>80.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>7</td>\n",
       "      <td>342</td>\n",
       "      <td>48.86</td>\n",
       "      <td>4.77</td>\n",
       "      <td>1.95</td>\n",
       "      <td>38.30</td>\n",
       "      <td>51.17</td>\n",
       "      <td>83.37</td>\n",
       "      <td>40.00</td>\n",
       "      <td>36.29</td>\n",
       "      <td>4.75</td>\n",
       "      <td>5.56</td>\n",
       "      <td>4</td>\n",
       "      <td>1.17</td>\n",
       "      <td>8.19</td>\n",
       "      <td>18.71</td>\n",
       "      <td>19.30</td>\n",
       "      <td>15.20</td>\n",
       "      <td>8.77</td>\n",
       "      <td>2.63</td>\n",
       "      <td>8.77</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3970</th>\n",
       "      <td>Education</td>\n",
       "      <td>el debate sobre los deberes llega al congreso ...</td>\n",
       "      <td>el debate sobre los deberes llega al congreso ...</td>\n",
       "      <td>11</td>\n",
       "      <td>4.73</td>\n",
       "      <td>2.00</td>\n",
       "      <td>81.82</td>\n",
       "      <td>90.91</td>\n",
       "      <td>33.88</td>\n",
       "      <td>23</td>\n",
       "      <td>1036</td>\n",
       "      <td>45.04</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.75</td>\n",
       "      <td>24.23</td>\n",
       "      <td>35.23</td>\n",
       "      <td>56.55</td>\n",
       "      <td>55.90</td>\n",
       "      <td>52.47</td>\n",
       "      <td>5.13</td>\n",
       "      <td>2.32</td>\n",
       "      <td>48</td>\n",
       "      <td>4.63</td>\n",
       "      <td>3.96</td>\n",
       "      <td>19.59</td>\n",
       "      <td>14.96</td>\n",
       "      <td>14.77</td>\n",
       "      <td>13.71</td>\n",
       "      <td>1.45</td>\n",
       "      <td>8.49</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>Society</td>\n",
       "      <td>como un paso decisivo en la descongestión judi...</td>\n",
       "      <td>tuiteros emberracados podrán juzgar algunos de...</td>\n",
       "      <td>6</td>\n",
       "      <td>7.67</td>\n",
       "      <td>3.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "      <td>447</td>\n",
       "      <td>49.67</td>\n",
       "      <td>4.46</td>\n",
       "      <td>1.84</td>\n",
       "      <td>43.18</td>\n",
       "      <td>53.47</td>\n",
       "      <td>92.41</td>\n",
       "      <td>45.78</td>\n",
       "      <td>42.46</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.34</td>\n",
       "      <td>8</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.57</td>\n",
       "      <td>22.37</td>\n",
       "      <td>15.66</td>\n",
       "      <td>11.41</td>\n",
       "      <td>9.84</td>\n",
       "      <td>3.36</td>\n",
       "      <td>8.95</td>\n",
       "      <td>4.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3972</th>\n",
       "      <td>Science</td>\n",
       "      <td>muy preocupada se declaró la secretaría de sal...</td>\n",
       "      <td>ojear pantallas ajenas en transmilenio ya es u...</td>\n",
       "      <td>9</td>\n",
       "      <td>5.44</td>\n",
       "      <td>2.33</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19</td>\n",
       "      <td>475</td>\n",
       "      <td>25.00</td>\n",
       "      <td>4.22</td>\n",
       "      <td>1.75</td>\n",
       "      <td>42.11</td>\n",
       "      <td>53.26</td>\n",
       "      <td>73.15</td>\n",
       "      <td>76.34</td>\n",
       "      <td>72.71</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2.32</td>\n",
       "      <td>6</td>\n",
       "      <td>1.26</td>\n",
       "      <td>3.16</td>\n",
       "      <td>16.00</td>\n",
       "      <td>14.74</td>\n",
       "      <td>10.95</td>\n",
       "      <td>11.79</td>\n",
       "      <td>7.58</td>\n",
       "      <td>11.79</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3973</th>\n",
       "      <td>Politics</td>\n",
       "      <td>el portavoz del grupo parlamentario del partid...</td>\n",
       "      <td>aliaga (par): el gobierno de lambán es \"débil\"...</td>\n",
       "      <td>19</td>\n",
       "      <td>3.68</td>\n",
       "      <td>1.47</td>\n",
       "      <td>89.47</td>\n",
       "      <td>94.74</td>\n",
       "      <td>101.08</td>\n",
       "      <td>15</td>\n",
       "      <td>624</td>\n",
       "      <td>41.60</td>\n",
       "      <td>4.33</td>\n",
       "      <td>1.79</td>\n",
       "      <td>30.93</td>\n",
       "      <td>39.90</td>\n",
       "      <td>44.45</td>\n",
       "      <td>57.01</td>\n",
       "      <td>53.91</td>\n",
       "      <td>2.92</td>\n",
       "      <td>3.69</td>\n",
       "      <td>61</td>\n",
       "      <td>9.78</td>\n",
       "      <td>4.97</td>\n",
       "      <td>17.15</td>\n",
       "      <td>13.62</td>\n",
       "      <td>12.34</td>\n",
       "      <td>17.63</td>\n",
       "      <td>1.12</td>\n",
       "      <td>8.81</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3974 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic                                               text  \\\n",
       "0       Science  la nasa recupera el contacto con un satélite d...   \n",
       "1       Economy  amlo aceleraría el consumo y el crecimiento ec...   \n",
       "2         Sport  al borde de un colapso nervioso quedó el hábil...   \n",
       "3      Politics  mediante pupitrazo de último minuto anoche, el...   \n",
       "4      Politics  muy temprano esta mañana el expresidente y aho...   \n",
       "...         ...                                                ...   \n",
       "3969      Sport  fifa levanta sanción a messi para regresar a l...   \n",
       "3970  Education  el debate sobre los deberes llega al congreso ...   \n",
       "3971    Society  como un paso decisivo en la descongestión judi...   \n",
       "3972    Science  muy preocupada se declaró la secretaría de sal...   \n",
       "3973   Politics  el portavoz del grupo parlamentario del partid...   \n",
       "\n",
       "                                               headline  words_h  word_size_h  \\\n",
       "0     la nasa recupera el contacto con un satélite d...       16         5.38   \n",
       "1     amlo aceleraría el consumo y el crecimiento ec...       11         5.27   \n",
       "2     compañero de james se ‘calvea’ y le juega pesa...       12         3.75   \n",
       "3     dian gravará este año a los niños que recojan ...       12         4.58   \n",
       "4            uribe asegura que insultó 358 guerrilleros        6         6.17   \n",
       "...                                                 ...      ...          ...   \n",
       "3969  fifa levanta sanción a messi para regresar a l...       10         5.10   \n",
       "3970  el debate sobre los deberes llega al congreso ...       11         4.73   \n",
       "3971  tuiteros emberracados podrán juzgar algunos de...        6         7.67   \n",
       "3972  ojear pantallas ajenas en transmilenio ya es u...        9         5.44   \n",
       "3973  aliaga (par): el gobierno de lambán es \"débil\"...       19         3.68   \n",
       "\n",
       "      avg_syllables_word_h  unique_words_h   ttr_h  mltd_h  sents  words  \\\n",
       "0                     2.50           87.50   93.75   71.68     16    479   \n",
       "1                     2.55           81.82   90.91   33.88      5    206   \n",
       "2                     1.67          100.00  100.00    0.00     13    368   \n",
       "3                     1.92          100.00  100.00    0.00      8    269   \n",
       "4                     2.50          100.00  100.00    0.00      9    260   \n",
       "...                    ...             ...     ...     ...    ...    ...   \n",
       "3969                  2.30           80.00   90.00   28.00      7    342   \n",
       "3970                  2.00           81.82   90.91   33.88     23   1036   \n",
       "3971                  3.00          100.00  100.00    0.00      9    447   \n",
       "3972                  2.33          100.00  100.00    0.00     19    475   \n",
       "3973                  1.47           89.47   94.74  101.08     15    624   \n",
       "\n",
       "      avg_words_sent  avg_word_size  avg_syllables_word  unique_words    ttr  \\\n",
       "0              29.94           4.76                2.04         31.52  43.22   \n",
       "1              41.20           4.58                1.89         33.98  49.03   \n",
       "2              28.31           4.23                1.70         41.58  53.26   \n",
       "3              33.62           4.75                1.92         40.15  53.53   \n",
       "4              28.89           4.18                1.59         49.62  61.54   \n",
       "...              ...            ...                 ...           ...    ...   \n",
       "3969           48.86           4.77                1.95         38.30  51.17   \n",
       "3970           45.04           4.30                1.75         24.23  35.23   \n",
       "3971           49.67           4.46                1.84         43.18  53.47   \n",
       "3972           25.00           4.22                1.75         42.11  53.26   \n",
       "3973           41.60           4.33                1.79         30.93  39.90   \n",
       "\n",
       "       mltd  huerta_score  szigriszt_score  upper_case_ratio  entity_ratio  \\\n",
       "0     56.66         53.90            49.96              3.87          2.71   \n",
       "1     57.83         51.42            47.69              4.35          5.83   \n",
       "2     64.43         75.96            72.72              2.60          3.80   \n",
       "3     65.16         57.35            53.71              2.02          1.49   \n",
       "4     93.74         81.97            78.75              1.81          2.69   \n",
       "...     ...           ...              ...               ...           ...   \n",
       "3969  83.37         40.00            36.29              4.75          5.56   \n",
       "3970  56.55         55.90            52.47              5.13          2.32   \n",
       "3971  92.41         45.78            42.46              0.88          1.34   \n",
       "3972  73.15         76.34            72.71              2.07          2.32   \n",
       "3973  44.45         57.01            53.91              2.92          3.69   \n",
       "\n",
       "      quotes  quotes_ratio  propn_ratio  noun_ratio  adp_ratio  det_ratio  \\\n",
       "0          0          0.00         4.80       24.43      17.75      15.66   \n",
       "1          2          0.97         9.22       22.82      15.53      16.50   \n",
       "2          7          1.90         7.34       16.58      15.22      13.32   \n",
       "3          4          1.49         2.23       22.30      19.33      15.24   \n",
       "4          7          2.69         5.77       17.31       8.46      10.77   \n",
       "...      ...           ...          ...         ...        ...        ...   \n",
       "3969       4          1.17         8.19       18.71      19.30      15.20   \n",
       "3970      48          4.63         3.96       19.59      14.96      14.77   \n",
       "3971       8          1.79         1.57       22.37      15.66      11.41   \n",
       "3972       6          1.26         3.16       16.00      14.74      10.95   \n",
       "3973      61          9.78         4.97       17.15      13.62      12.34   \n",
       "\n",
       "      punct_ratio  pron_ratio  verb_ratio  adv_ratio  sym_ratio  label  \n",
       "0            7.10        1.67        8.35       3.13       0.00      1  \n",
       "1           11.17        0.49        6.31       0.97       0.00      1  \n",
       "2           13.04        4.35        8.15       3.53       0.54      0  \n",
       "3           10.04        0.74        6.69       3.35       0.00      0  \n",
       "4           20.00        5.38       10.38       5.00       0.77      0  \n",
       "...           ...         ...         ...        ...        ...    ...  \n",
       "3969         8.77        2.63        8.77       2.34       0.00      1  \n",
       "3970        13.71        1.45        8.49       2.70       0.48      1  \n",
       "3971         9.84        3.36        8.95       4.25       0.00      0  \n",
       "3972        11.79        7.58       11.79       3.37       0.63      0  \n",
       "3973        17.63        1.12        8.81       2.56       0.00      1  \n",
       "\n",
       "[3974 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature extraction function for predictions\n",
    "\n",
    "To make predictions we need to extracte features from a given news headline and news text content. So we are going to pack the code above to extract the features for our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 36.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk import FreqDist\n",
    "from lexical_diversity import lex_div as ld\n",
    "\n",
    "def get_news_features(headline, text):\n",
    "    \n",
    "    nlp = spacy.load('es_core_news_lg')\n",
    "\n",
    "    ## headline ##\n",
    "    headline = re.sub(r\"http\\S+\", \"\", headline)\n",
    "    headline = re.sub(r\"http\", \"\", headline)\n",
    "    headline = re.sub(r\"@\\S+\", \"\", headline)\n",
    "    headline = re.sub(\"\\n\", \" \", headline)\n",
    "    headline = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", headline)\n",
    "    headline = headline.replace(r\"*NUMBER*\", \"número\")\n",
    "    headline = headline.replace(r\"*PHONE*\", \"número\")\n",
    "    headline = headline.replace(r\"*EMAIL*\", \"email\")\n",
    "    headline = headline.replace(r\"*URL*\", \"url\")\n",
    "    headline_new = headline.lower()\n",
    "    doc_h = nlp(headline_new)\n",
    "\n",
    "    list_tokens_h = []\n",
    "    list_tags_h = []\n",
    "\n",
    "    for sentence_h in doc_h.sents:\n",
    "        for token in sentence_h:\n",
    "            list_tokens_h.append(token.text)\n",
    "\n",
    "    fdist_h = FreqDist(list_tokens_h)\n",
    "    syllables_h = get_nsyllables(headline)\n",
    "    words_h = len(list_tokens_h)\n",
    "\n",
    "    # headline complexity features\n",
    "    avg_word_size_h = round(sum(len(word) for word in list_tokens_h) / words_h, 2)\n",
    "    avg_syllables_word_h = round(syllables_h / words_h, 2)\n",
    "    unique_words_h = round((len(fdist_h.hapaxes()) / words_h) * 100, 2)\n",
    "    mltd_h = round(ld.mtld(list_tokens_h), 2)\n",
    "    ttr_h = round(ld.ttr(list_tokens_h) * 100, 2)\n",
    "\n",
    "    ## text content##     \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"http\", \"\", text)\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    text = text.replace(r\"*NUMBER*\", \"número\")\n",
    "    text = text.replace(r\"*PHONE*\", \"número\")\n",
    "    text = text.replace(r\"*EMAIL*\", \"email\")\n",
    "    text = text.replace(r\"*URL*\", \"url\")\n",
    "\n",
    "    # to later calculate upper case letters ratio\n",
    "    alph = list(filter(str.isalpha, text))\n",
    "    text_lower = text.lower()\n",
    "    doc = nlp(text_lower)\n",
    "\n",
    "    list_tokens = []\n",
    "    list_pos = []\n",
    "    list_tag = []\n",
    "    list_entities = []\n",
    "    sents = 0\n",
    "\n",
    "    for entity in doc.ents:\n",
    "        list_entities.append(entity.label_)\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        sents += 1\n",
    "        for token in sentence:\n",
    "            list_tokens.append(token.text)\n",
    "            list_pos.append(token.pos_)\n",
    "            list_tag.append(token.tag_)\n",
    "\n",
    "    # Calculate entities, pos, tag, freq, syllables, words and quotes\n",
    "    entities = len(list_entities)\n",
    "    n_pos = nltk.Counter(list_pos)\n",
    "    n_tag = nltk.Counter(list_tag)\n",
    "    fdist = FreqDist(list_tokens)\n",
    "    syllables = get_nsyllables(text)\n",
    "    words = len(list_tokens)\n",
    "    quotes = n_tag['PUNCT__PunctType=Quot']\n",
    "\n",
    "    # complexity features\n",
    "    avg_word_sentence = round(words / sents, 2)\n",
    "    avg_word_size = round(sum(len(word) for word in list_tokens) / words, 2)\n",
    "    avg_syllables_word = round(syllables / words, 2)\n",
    "    unique_words = round((len(fdist.hapaxes()) / words) * 100, 2)\n",
    "    ttr = round(ld.ttr(list_tokens) * 100, 2)\n",
    "\n",
    "    # readability spanish test\n",
    "    huerta_score = round(206.84 - (60 * avg_syllables_word) - (1.02 * avg_word_sentence), 2)\n",
    "    szigriszt_score = round(206.835 - ((62.3 * syllables) / words) - (words / sents), 2)\n",
    "\n",
    "    # stylometric features\n",
    "    mltd = round(ld.mtld(list_tokens), 2)\n",
    "    upper_case_ratio = round(sum(map(str.isupper, alph)) / len(alph) * 100, 2)\n",
    "    entity_ratio = round((entities / words) * 100, 2)\n",
    "    quotes_ratio = round((quotes / words) * 100, 2)\n",
    "    propn_ratio = round((n_pos['PROPN'] / words) * 100 , 2)\n",
    "    noun_ratio = round((n_pos['NOUN'] / words) * 100, 2) \n",
    "    pron_ratio = round((n_pos['PRON'] / words) * 100, 2)\n",
    "    adp_ratio = round((n_pos['ADP'] / words) * 100, 2)\n",
    "    det_ratio = round((n_pos['DET'] / words) * 100, 2)\n",
    "    punct_ratio = round((n_pos['PUNCT'] / words) * 100, 2)\n",
    "    verb_ratio = round((n_pos['VERB'] / words) * 100, 2)\n",
    "    adv_ratio = round((n_pos['ADV'] / words) * 100, 2)\n",
    "    sym_ratio = round((n_tag['SYM'] / words) * 100, 2)\n",
    "\n",
    "    # create df_features\n",
    "    df_features = pd.DataFrame({'words_h': words_h, 'word_size_h': [avg_word_size_h],'avg_syllables_word_h': [avg_syllables_word_h],\n",
    "                                'unique_words_h': [unique_words_h], 'ttr_h': ttr_h, 'mltd_h': [mltd_h], 'sents': sents, 'words': words,\n",
    "                                'avg_words_sent': [avg_word_sentence], 'avg_word_size': [avg_word_size], \n",
    "                                'avg_syllables_word': avg_syllables_word, 'unique_words': [unique_words], \n",
    "                                'ttr': [ttr], 'huerta_score': [huerta_score], 'szigriszt_score': [szigriszt_score],\n",
    "                                'mltd': [mltd], 'upper_case_ratio': [upper_case_ratio], 'entity_ratio': [entity_ratio],\n",
    "                                'quotes': quotes, 'quotes_ratio': [quotes_ratio], 'propn_ratio': [propn_ratio], \n",
    "                                'noun_ratio': [noun_ratio], 'pron_ratio': [pron_ratio], 'adp_ratio': [adp_ratio],\n",
    "                                'det_ratio': [det_ratio], 'punct_ratio': [punct_ratio], 'verb_ratio': [verb_ratio],\n",
    "                                'adv_ratio': [adv_ratio], 'sym_ratio': [sym_ratio]})\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert news headline:El Gobierno ha presentado hoy al Niño de Schrödinger, que va y no va al colegio\n",
      "insert news content:La ministra de Educación y Formación Profesional, Isabel Celaá, ha presentado esta mañana al Niño de Schrödinger, fruto de un proyecto en el que han colaborado varias universidades españolas y que viene a resolver el problema de la vuelta a los colegios en plena ola de contagios por coronavirus.  «Va y no va al colegio y está expuesto al virus pero al mismo tiempo no lo está», ha explicado Celaá, insistiendo en que se trata de «una paradoja avalada científicamente».  La ministra ha mostrado a los medios al niño, cuyo nombre es Fernando Campos Leza, describiéndolo como «un alumno perfectamente sano y normal que ahora mismo, estando aquí con nosotros, está al mismo tiempo en casa, donde permanecerá mientras vaya al colegio con normalidad junto al resto de niños de Schrödinger».  A partir de mañana y hasta el inicio del nuevo curso escolar, los padres deberán adaptar a sus hijos para que sean también niños de Schrödinger. Para ello, se les facilitará y no se les facilitará una caja.  Esta misma tarde, la ministra de Trabajo, Yolanda Díaz, presentará a los padres de Schrödinger, capaces de compatibilizar sus obligaciones profesionales con las familiares yendo a la oficina y al mismo tiempo quedándose en su domicilio particular para mayor seguridad, atendiendo y no atendiendo a sus niños de Schrödinger mientras permanecen en casa y en clase.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_h</th>\n",
       "      <th>avg_word_size_h</th>\n",
       "      <th>avg_syllables_word</th>\n",
       "      <th>unique_words_h</th>\n",
       "      <th>mltd_h</th>\n",
       "      <th>sents</th>\n",
       "      <th>words</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>avg_word_size</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>ttr</th>\n",
       "      <th>huerta_score</th>\n",
       "      <th>szigriszt_score</th>\n",
       "      <th>mltd</th>\n",
       "      <th>upper_case_ratio</th>\n",
       "      <th>entity_ratio</th>\n",
       "      <th>quotes</th>\n",
       "      <th>quotes_ratio</th>\n",
       "      <th>propn_ratio</th>\n",
       "      <th>noun_ratio</th>\n",
       "      <th>pron_ratio</th>\n",
       "      <th>adp_ratio</th>\n",
       "      <th>det_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>verb_ratio</th>\n",
       "      <th>adv_ratio</th>\n",
       "      <th>sym_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>258</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.47</td>\n",
       "      <td>35.66</td>\n",
       "      <td>65.99</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.76</td>\n",
       "      <td>76.47</td>\n",
       "      <td>88.24</td>\n",
       "      <td>101.3</td>\n",
       "      <td>98.22</td>\n",
       "      <td>40.46</td>\n",
       "      <td>6.35</td>\n",
       "      <td>11.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.65</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.65</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.88</td>\n",
       "      <td>17.65</td>\n",
       "      <td>11.76</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   words_h  avg_word_size_h  avg_syllables_word  unique_words_h  mltd_h  \\\n",
       "0      258              4.4                1.47           35.66   65.99   \n",
       "\n",
       "   sents  words  avg_word_sentence  avg_word_size  unique_words    ttr  \\\n",
       "0      1     17               17.0           3.76         76.47  88.24   \n",
       "\n",
       "   huerta_score  szigriszt_score   mltd  upper_case_ratio  entity_ratio  \\\n",
       "0         101.3            98.22  40.46              6.35         11.76   \n",
       "\n",
       "   quotes  quotes_ratio  propn_ratio  noun_ratio  pron_ratio  adp_ratio  \\\n",
       "0       0           0.0        17.65        5.88         0.0      17.65   \n",
       "\n",
       "   det_ratio  punct_ratio  verb_ratio  adv_ratio  sym_ratio  \n",
       "0       5.88         5.88       17.65      11.76        0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline = input('Insert news headline:')\n",
    "text = input('Insert news content:')\n",
    "\n",
    "get_news_features(text, headline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
